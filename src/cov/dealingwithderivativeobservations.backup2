#ifndef _ISOTROPIC_COVARIANCE_FUNCTION_DEALING_WITH_DERIVATIVE_OBSERVATIONS_HPP_
#define _ISOTROPIC_COVARIANCE_FUNCTION_DEALING_WITH_DERIVATIVE_OBSERVATIONS_HPP_

#include "isotropic.hpp"
#include "../data/derivativetrainingdata.hpp"

namespace GP {

 /**
	* @class		IsotropicWithPartialDerivativesWRTInput
	* @brief		Isotropic covariance function dealing with derivative observations.
	*
	*				Covariance functions of partial derivatives with respective to input coordinates
	*				are partial derivatives of covariance functions with respective to input coordinates.
	*
	*				k(x, x') = k(r) = sigma_f^2 f(s), s = s(r), r = |x-x'|
	*
	*				(1) Partial derivatives with respective to input coordinates
	*
	*                      dx           dk(x, x')     dk      ds
	*              (a) k(------, dx') = ---------- = ---- * ------
	*                     dx_i             dx_i       ds     dx_i
	*               
	*                          dx'       dk(x, x')     dk      ds
	*              (b) k(dx, -------) = ---------- = ---- * --------
	*                         dx'_j        dx'_j       ds     dx'_j
	*
	*                      dx      dx'       dk(x, x')      d^2k    ds    ds      dk     d^2s
	*              (c) k(------, -------) = ------------ = ------*-----*------ + ----*-----------
	*                     dx_i    dx'_j      dx_i dx'_j     ds^2   dx_i  dx'_j    ds   dx_i dx'_j
	*
	*					which requires four components, ds/dx_i, ds/dx'_j, d^2k/ds^2 and d^2s/dx_i dx'_j
	*					as well as dk/ds which is already required in Isotropic.
	*
	*				(2) Partial derivatives with respective to input coordinates and hyperparameters for learning
	*
	*                        d
	*              (0) -------------- k(?x, ?x') = 2 * k(?x, ?x')
	*                   dlog(sigma_f)
	*               
	*                       d         dx             d^2 k(x, x')           d^2k    ds     ds      dk     d^2s
	*              (a) ---------- k(------, dx') = ---------------- = ell*(------*------*------ + ----*----------)
	*                   dlog(ell)    dx_i           dlog(ell) dx_i          ds^2   dell   dx_i     ds   dell dx_i 
	*               
	*                       d            dx'        d^2 k(x, x')           d^2k    ds     ds       dk     d^2s
	*              (b) ---------- k(x, ------) = ----------------- = ell*(------*------*------- + ----*-----------)
	*                   dlog(ell)       dx'_j     dlog(ell) dx'_j          ds^2   dell   dx'_j     ds   dell dx'_j 
	*
	*                       d         dx      dx'            d^3k    ds     ds    ds      d^2k     d^2s      ds       d^2k    ds      d^2s        d^2k    ds      d^2s        dk        d^3s
	*              (c) ---------- k(------, -------) = ell*(------*------*-----*------ + ------*----------*------- + ------*-----*------------ + ------*------*----------- + ----*-----------------)
	*                   dlog(ell)    dx_i    dx'_j           ds^3   dell   dx_i  dx'_j    ds^2   dell dx_i  dx'_j     ds^2   dx_i  dell dx'_j     ds^2   dell   dx_i dx'_j    ds   dell dx_i dx'_j
	*
	*                                                        d^3k    ds     ds    ds      d^2k      d^2s      ds        ds      d^2s          ds      d^2s         dk        d^3s
	*                                                = ell*(------*------*-----*------ + ------*(----------*------- + ------*------------ + ------*-----------) + ----*-----------------)
	*                                                        ds^3   dell   dx_i  dx'_j    ds^2    dell dx_i  dx'_j     dx_i  dell dx'_j      dell   dx_i dx'_j     ds   dell dx_i dx'_j
	*
	*					which additionally requires four components, d^2s/dell dx_i, d^2s/dell dx'_j, d^3k/ds^3 and d^3s/dell dx_i dx'_j
	*					as well as ds/dell which are already required in Isotropic.
	* @author	Soohwan Kim
	* @date		10/06/2014
	*/
template<typename Scalar, template<typename> class Cov>
class IsotropicWithPartialDerivativesWRTInput : public Isotropic<Scalar, Cov>
{
protected:
	// enumeration for data pairs
	enum DataPair { X_X, X_Xd, Xd_X, Xd_Xd };

public:
	/**
	 * @brief	K: Self covariance matrix between the training data.
	 * 			[(K), K* ]: covariance matrix of the marginal Gaussian distribution 
	 * 			[K*T, K**]
	 *          supports three calculations: K, dK_dlog(ell), and dK_dlog(sigma_f)
	 * @note		CRTP (Curiously Recursive Template Pattern)
	 *				This class takes the corresponding covariance function class as a template.
	 *				Thus, Cov<Scalar>::f(), Cov<Scalar>::s(), Cov<Scalar>::dk_ds(), and Cov<Scalar>::ds_dell()
	 *				should be accessable from this class.
	 *				In other words, they should be public, or this class and Cov<Scalar> should be friends.
	 * @param	[in] logHyp 			The log hyperparameters, log([ell, sigma_f]).
	 * @param	[in] trainingData 	The training data.
	 * @param	[in] pdHypIndex		(Optional) Hyperparameter index.
	 * 										It returns the partial derivatives of the covariance matrix
	 * 										with respect to this hyperparameter. 
	 * 										The partial derivatives are required for learning hyperparameters.
	 * 										(Example) pdHypIndex = 0: pd[K]/pd[log(ell)], pdHypIndex = 1: pd[K]/pd[log(sigma_f)]
	 * 										(Default = -1) K
	 * @return	An NxN matrix pointer.
	 * 			N: The number of training data.
	 */
	static MatrixPtr K(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const int pdHypIndex = -1) 
	{
		// Output
		// K: (N + Nd*(D+1)) x (N + Nd*(D+1))
		// 
		// for example, when D = 3
		//                 |  F (N) | Fd (Nd) | Df1 (Nd) | Df2 (Nd) | Df3 (Nd) |
		// K = ----------------------------------------------------------------
		//        F   (N)  |    F-F |     F-Fd,     F-Df1,     F-Df2,      F-Df3
		//       --------------------------------------------------------------
		//        Fd  (Nd) |      - |    Fd-Fd,    Fd-Df1,    Fd-Df2,     Fd-Df3
		//        Df1 (Nd) |      - |        -,   Df1-Df1,   Df1-Df2,    Df1-Df3
		//        Df2 (Nd) |      - |        -,         -,   Df2-Df2,    Df2-Df3
		//        Df3 (Nd) |      - |        -,         -,         -,    Df3-Df3


		// constants		
		const int N		= derivativeTrainingData.N();		// number of function training data
		const int Nd	= derivativeTrainingData.Nd();	// number of derivative training data
		const int D		= derivativeTrainingData.D();		// number of dimensions
		const int NN	= N + Nd*(D+1);

		// assertion
		assert(pdHypIndex < logHyp.size());
		assert(N > 0 || Nd > 0);

		// memory allocation
		MatrixPtr pK(new Matrix(NN, NN));

		// fill the block matrix
		int startRow,  startCol;
		int blockRows, blockCols;
		for(int row = 0; row < D+2; row++)
		{
			// size
			if(row == 0)	{ if(N  <= 0) continue;		blockRows = N;		startRow = 0;					}
			else				{ if(Nd <= 0) continue;		blockRows = Nd;	startRow = N + (row-1)*Nd;	}

			for(int col = 0; col < D+2; col++)
			{
				// size
				if(col == 0)	{ if(N  <= 0) continue;	blockCols = N;		startCol = 0;					}	
				else				{ if(Nd <= 0) continue;	blockCols = Nd;	startCol = N + (col-1)*Nd;	}

				// block-wise symmetric
				if(row > col) 
				{
					pK->block(startRow, startCol, blockRows, blockCols) = pK->block().transpose();
					continue;
				}

				switch(row)
				{
				// first row
				case 0:
				{
					switch(col)
					{
					case 0:	pK->block(startRow, startCol, blockRows, blockCols) = *(k(logHyp, derivativeTrainingData.sqDist(), pdIndex));						break;
					case 1:	pK->block(startRow, startCol, blockRows, blockCols) = *(k(logHyp, derivativeTrainingData.sqDistXXd(), pdIndex));					break;
					default:	pK->block(startRow, startCol, blockRows, blockCols) = *(dk_dx(logHyp, derivativeTrainingData.sqDistXXd(), col-2, pdIndex));
					}
					break;
				}

				// second row
				case 1:
				{
					switch(col)
					{
					case 1:	pK->block(startRow, startCol, blockRows, blockCols) = *(k(logHyp, derivativeTrainingData.sqDistXd(), pdIndex));					break;
					default:	pK->block(startRow, startCol, blockRows, blockCols) = *(dk_dx(logHyp, derivativeTrainingData.sqDistXd(), col-2, pdIndex));
					}
					break;
				}

				// other rows
				default:		pK->block(startRow, startCol, blockRows, blockCols) = *(d2k_dx2(logHyp, derivativeTrainingData.sqDistXd(), row-2, col-2, pdIndex));
				}
			}
		}



			}
		}



		//
		// 1. first row
		//

		// initialize row, col
		int startingRow = 0;
		int startingCol = 0;
		if(N > 0)
		{
			// 1.a first column
			pK->block(startingRow, startingCol, N, N) = *(k(logHyp, derivativeTrainingData, X_X, pdIndex));
			startingCol += N;

			// second and other columns
			if(Nd > 0)
			{
				// 1.b second column
				pK->block(startingRow, startingCol, N, Nd) = *(k(logHyp, derivativeTrainingData, X_Xd, pdIndex));
				startingCol += Nd;

				// 1.c other columns
				for(int col = 0; col < D; col++)
				{
					pK->block(startingRow, startingCol, N, Nd) = *(K_F_Df(logHyp, derivativeTrainingData, col, pdIndex));
					startingCol += Nd;
				}
			}

			// finalize the first row
			startingRow += N;
		}

		// second and other rows
		if(Nd > 0)
		{
			//
			// 2. second row
			//

			// initialize the col
			startingCol = 0;

			// 2.a first column
			if(N > 0)
			{
				pK->block(startingRow, startingCol, Nd, N) = *(k(logHyp, derivativeTrainingData, Xd_X, pdIndex));
				startingCol += N;
			}

			// 2.b second column
			pK->block(startingRow, startingCol, Nd, Nd) = *(k(logHyp, derivativeTrainingData, Xd_Xd, pdIndex));
			startingCol += Nd;

			// 2.c other columns
			for(int col = 0; col < D; col++)
			{
				pK->block(startingRow, startingCol, Nd, Nd) = *(K_Fd_Df(logHyp, derivativeTrainingData, col, pdIndex));
				startingCol += Nd;
			}

			// finalize the second row
			startingRow += Nd;

			//
			// 3. other rows
			//
			for(int row = 0; row < D; row++)
			{
				// initialize the col
				startingCol = 0;

				// 3.a first column
				if(N > 0)
				{
					pK->block(startingRow, startingCol, Nd, N) = *(K_Df_F(logHyp, derivativeTrainingData, row, pdIndex));
					startingCol += N;
				}

				// 3.b second column
				pK->block(startingRow, startingCol, Nd, Nd) = *(K_Df_Fd(logHyp, derivativeTrainingData, row, pdIndex));
				startingCol += Nd;

				// 3.c other columns
				for(int col = 0; col < D; col++)
				{
					pK->block(startingRow, startingCol, Nd, Nd) = *(K_Df_Df(logHyp, derivativeTrainingData, row, col, pdIndex));
					startingCol += Nd;
				}

				// finalize the row
				startingRow += Nd;
			}
		}

		return pK;
	}

protected:
	// K(F(X),  F(X))
	// K(F(X),  F(Xd))
	// K(F(Xd), F(X))
	// K(F(Xd), F(Xd))
	static MatrixPtr k(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const DataPair dataPair, const int pdIndex)
	{
		// K(Xd, X) = K(X, Xd)T
		if(dataPair == Xd_X)	
		{
			MatrixPtr pK = k(logHyp, derivativeTrainingData, X_Xd, pdIndex);
			pK->transposeInPlace(); // transpose
			return pK;
			break;
		}

		// r^2: squared distances which depends on the data pair
		MatrixConstPtr pSqDist;
		switch(dataPair)
		{
			// r^2(x, x) for k(x, x)
			case X_X:	
			{
				pSqDist = derivativeTrainingData.sqDist();
				break;
			}

			// r^2(x, xd) for k(x, xd)
			case X_Xd:
			{
				pSqDist = derivativeTrainingData.sqDistXXd();
				break;
			}

			// r^2(xd, xd) for k(xd, xd)
			case Xd_Xd:
			{
				pSqDist = derivativeTrainingData.sqDistXd();
				break;
			}
		}

		return k(logHyp, pSqDist, pdIndex);
	}

	// K(dF(Xd)/dx_i, F(X))
	// K(F(X), dF(Xd)/dx_j)
	static MatrixPtr dk_dx(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const DataPair dataPair, const coord, const int pdIndex)
	{
		// assertion
		assert(dataPair == X_Xd || dataPair == Xd_X);

		// K(dF(Xd)/dx_j, F(X)) = K(F(X), dF(Xd)/dx_j)T
		if(dataPair == Xd_X)	
		{
			MatrixPtr pK = k(logHyp, derivativeTrainingData, X_Xd, coord, pdIndex);
			pK->transposeInPlace(); // transpose
			return pK;
			break;
		}

		// constants		
		const int N		= derivativeTrainingData.N();		// number of function training data
		const int Nd	= derivativeTrainingData.Nd();	// number of derivative training data

		// r^2: squared distances which depends on the data pair
		MatrixConstPtr pSqDist;
		switch(dataPair)
		{
			// r^2(x, x) for k(x, x)
			case X_X:	
			{
				return K(logHyp, static_cast<TrainingData<Scalar> >(derivativeTrainingData), pdIndex);
			}

			// r^2(x, xd) for k(x, xd)
			case X_Xd:
			{
				pSqDist = derivativeTrainingData.sqDistXXd();
				break;
			}

			// r^2(xd, x) for k(xd, x)
			case Xd_X:	
			{
				MatrixPtr pK = K_FF(logHyp, derivativeTrainingData, X_Xd, pdIndex);
				pK->transposeInPlace();
				return pK;
				break;
			}

			// r^2(xd, xd) for k(xd, xd)
			case Xd_Xd:
			{
				pSqDist = derivativeTrainingData.sqDistXd();
				break;
			}
		}

		// derivative of K w.r.t a hyperparameter
		switch(pdHypIndex)
		{
			// derivative w.r.t log(ell)
			//      d         dx             d^2 k(x, x')           d^2k    ds     ds      dk     d^2s
			// ---------- k(------, dx') = ---------------- = ell*(------*------*------ + ----*----------)
			//  dlog(ell)    dx_i           dlog(ell) dx_i          ds^2   dell   dx_i     ds   dell dx_i 
			case 0:
			{
				// constants
				const Scalar ell = exp(logHyp(0)); // ell

				// memory allocation
				MatrixPtr pK(new Matrix(pSqDist->rows(), pSqDist->cols()));

				// components
				MatrixPtr pd2k_ds2		= Cov<Scalar>::d2k_ds2(logHyp, pSqDist);
				MatrixPtr pds_dell		= Cov<Scalar>::ds_dell(logHyp, pSqDist);
				MatrixPtr pds_dxi			= Cov<Scalar>::ds_dxi(logHyp, pSqDist);
				MatrixPtr pdk_ds			= Cov<Scalar>::dk_ds(logHyp, pSqDist);
				MatrixPtr pd2s_delldxi	= Cov<Scalar>::d2s_delldxi(logHyp, pSqDist);

				// calculation
				(*pK).noalias() = ell*(*(pd2k_ds2->cwiseProduct(*(pds_dell->cwiseProduct(*pds_dxi))))
					                  + *(dk_ds->cwiseProduct(*d2s_delldxi)));

				return pK;	
			}

			// derivative w.r.t log(sigma_f)
			//      d             dx                  dx 
			// -------------- k(------, x') = 2 * k(------, x')
			//  dlog(sigma_f)    dx_i                dx_i 
			case 1:
			{
				// memory allocation
				MatrixPtr pK = k(logHyp, pSqDist);

				// dk/dlog(sigma_f) = sigma_f * k(x, x')
				(*pK).noalias() = static_cast<Scalar>(2.0) * (*pK);

				return pK;
			}
		}

		// original dK/dx_i

		//     dx           dk(x, x')     dk      ds
		// k(------, dx') = ---------- = ---- * ------
		//    dx_i             dx_i       ds     dx_i
		// constants
		const Scalar sigma_f2 = exp(static_cast<Scalar>(2.0) * logHyp(1)); // sigma_f^2

		// memory allocation
		MatrixPtr pK = Cov<Scalar>::f(logHyp, derivativeTrainingData);

		// k(x, x') = sigma_f^2 * f(s)
		(*pK).noalias() = sigma_f2 * (*pK);

		return pK;
	}

	//         dx'          dx'
	// k(dx, -------) = k(-------, dx)
	//        dx'_j        dx'_j
	//
	static MatrixPtr dk_dxj(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const DataPair dataPair, const int coord_j, const int pdIndex)
	{
		MatrixPtr pK = dk_dxj(logHyp, derivativeTrainingData, dataPair, coord_j, pdIndex);
		pK->transposeInPlace(); // transpose
		return pK;
	}

	// K(dF(Xd)/dx_i, dF(Xd)/dx_j)
	static MatrixPtr d2k_dxidxj(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const int coord_j, const int coord_j, const int pdIndex)

	static MatrixPtr K_F_Df(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int col-1, const int pdIndex));
	static MatrixPtr K_Fd_F(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const int pdIndex));
	static MatrixPtr K_Fd_Fd(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const int pdIndex));
	static MatrixPtr K_Fd_Df(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int col-1, const int pdIndex));
	static MatrixPtr K_Df_F(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int row-1, const int pdIndex));
	static MatrixPtr K_Df_Fd(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int row-1, const int pdIndex));
	static MatrixPtr K_Df_Df(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int row-1, int col-1, const int pdIndex));

	/**
	 * @brief	K*: Cross covariance matrix between the training data and test data.
	 * 			[K,    (K*)]: covariance matrix of the marginal Gaussian distribution 
	 * 			[(K*T), K**]
	 * @note		No pdHypIndex parameter is passed,
	 * 			because the partial derivatives of the covariance matrix
	 * 			is only required for learning hyperparameters.
	 * @param	[in] logHyp 			The log hyperparameters, log([ell, sigma_f]).
	 * @param	[in] trainingData 	The training data.
	 * @param	[in] pXs 				The test inputs.
	 * @return	An NxM matrix pointer.
	 * 			N: The number of training data.
	 * 			M: The number of test data.
	 */
	static MatrixPtr Ks(const Hyp &logHyp, TrainingData<Scalar> &trainingData, const MatrixConstPtr pXs)
	{
		// constants
		const Scalar sigma_f2 = exp(static_cast<Scalar>(2.0) * logHyp(1)); // sigma_f^2

		// memory allocation
		MatrixPtr pM = Cov<Scalar>::fs(logHyp, trainingData, pXs);

		// k(x, x') = sigma_f^2 * f(s)
		(*pM).noalias() = sigma_f2 * (*pM);

		return pM;
	}

	/**
	 * @brief	K**: Self [co]variance matrix between the test data.
	 * 			[K,    K*  ]: covariance matrix of the marginal Gaussian distribution 
	 * 			[K*T, (K**)]
	 * @note		No pdHypIndex parameter is passed,
	 * 			because the partial derivatives of the covariance matrix
	 * 			is only required for learning hyperparameters.
	 * @param	[in] logHyp 			The log hyperparameters, log([ell, sigma_f]).
	 * @param	[in] pXs 				The test inputs.
	 * @param	[in] fVarianceVector	Flag for the return value.
	 * @return	fVarianceVector == true : An Mx1 matrix pointer.
	 * 			fVarianceVector == false: An MxM matrix pointer.
	 * 			M: The number of test data.
	 */
	static MatrixPtr Kss(const Hyp &logHyp, const MatrixConstPtr pXs, const bool fVarianceVector = true)
	{
		// constants
		const Scalar sigma_f2 = exp(static_cast<Scalar>(2.0) * logHyp(1)); // sigma_f^2

		// K: self-variance vector (Mx1).
		if(fVarianceVector)
		{			
			// memory allocation
			const int M = pXs->rows();			// number of test data.
			MatrixPtr pM(new Matrix(M, 1));	// Mx1 matrix

			// k(x, x') = sigma_f^2
			pM->fill(sigma_f2);

			return pM;
		}
		else
		{
			// memory allocation
			MatrixPtr pM = Cov<Scalar>::fss(logHyp, pXs);

			// k(x, x') = sigma_f^2 * f(s)
			(*pM).noalias() = sigma_f2 * (*pM);

			return pM;
		}
	}
};

}

#endif
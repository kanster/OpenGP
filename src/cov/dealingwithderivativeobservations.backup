#ifndef _ISOTROPIC_COVARIANCE_FUNCTION_DEALING_WITH_DERIVATIVE_OBSERVATIONS_HPP_
#define _ISOTROPIC_COVARIANCE_FUNCTION_DEALING_WITH_DERIVATIVE_OBSERVATIONS_HPP_

#include "isotropic.hpp"
#include "../data/derivativetrainingdata.hpp"

namespace GP {

 /**
	* @class		IsotropicWithPartialDerivativesWRTInput
	* @brief		Isotropic covariance function dealing with derivative observations.
	*
	*				Covariance functions of partial derivatives with respective to input coordinates
	*				are partial derivatives of covariance functions with respective to input coordinates.
	*
	*				k(x, x') = k(r) = sigma_f^2 f(s), s = s(r), r = |x-x'|
	*
	*				(1) Partial derivatives with respective to input coordinates
	*
	*                      dx           dk(x, x')     dk      ds
	*              (a) k(------, dx') = ---------- = ---- * ------
	*                     dx_i             dx_i       ds     dx_i
	*               
	*                          dx'       dk(x, x')     dk      ds
	*              (b) k(dx, -------) = ---------- = ---- * --------
	*                         dx'_j        dx'_j       ds     dx'_j
	*
	*                      dx      dx'       dk(x, x')      d^2k    ds    ds      dk     d^2s
	*              (c) k(------, -------) = ------------ = ------*-----*------ + ----*-----------
	*                     dx_i    dx'_j      dx_i dx'_j     ds^2   dx_i  dx'_j    ds   dx_i dx'_j
	*
	*					which requires four components, ds/dx_i, ds/dx'_j, d^2k/ds^2 and d^2s/dx_i dx'_j
	*					as well as dk/ds which is already required in Isotropic.
	*
	*				(2) Partial derivatives with respective to input coordinates and hyperparameters for learning
	*
	*                        d
	*              (0) -------------- k(?x, ?x') = 2 * k(?x, ?x')
	*                   dlog(sigma_f)
	*               
	*                       d         dx             d^2 k(x, x')           d^2k    ds     ds      dk     d^2s
	*              (a) ---------- k(------, dx') = ---------------- = ell*(------*------*------ + ----*----------)
	*                   dlog(ell)    dx_i           dlog(ell) dx_i          ds^2   dell   dx_i     ds   dell dx_i 
	*               
	*                       d            dx'        d^2 k(x, x')           d^2k    ds     ds       dk     d^2s
	*              (b) ---------- k(x, ------) = ----------------- = ell*(------*------*------- + ----*-----------)
	*                   dlog(ell)       dx'_j     dlog(ell) dx'_j          ds^2   dell   dx'_j     ds   dell dx'_j 
	*
	*                       d         dx      dx'            d^3k    ds     ds    ds      d^2k     d^2s      ds       d^2k    ds      d^2s        d^2k    ds      d^2s        dk        d^3s
	*              (c) ---------- k(------, -------) = ell*(------*------*-----*------ + ------*----------*------- + ------*-----*------------ + ------*------*----------- + ----*-----------------)
	*                   dlog(ell)    dx_i    dx'_j           ds^3   dell   dx_i  dx'_j    ds^2   dell dx_i  dx'_j     ds^2   dx_i  dell dx'_j     ds^2   dell   dx_i dx'_j    ds   dell dx_i dx'_j
	*
	*                                                        d^3k    ds     ds    ds      d^2k      d^2s      ds        ds      d^2s          ds      d^2s         dk        d^3s
	*                                                = ell*(------*------*-----*------ + ------*(----------*------- + ------*------------ + ------*-----------) + ----*-----------------)
	*                                                        ds^3   dell   dx_i  dx'_j    ds^2    dell dx_i  dx'_j     dx_i  dell dx'_j      dell   dx_i dx'_j     ds   dell dx_i dx'_j
	*
	*					which additionally requires four components, d^2s/dell dx_i, d^2s/dell dx'_j, d^3k/ds^3 and d^3s/dell dx_i dx'_j
	*					as well as ds/dell which are already required in Isotropic.
	* @author	Soohwan Kim
	* @date		10/06/2014
	*/
template<typename Scalar, template<typename> class Cov>
class IsotropicWithPartialDerivativesWRTInput : public Isotropic<Scalar, Cov>
{
	/**
	 * @brief	K: Self covariance matrix between the training data.
	 * 			[(K), K* ]: covariance matrix of the marginal Gaussian distribution 
	 * 			[K*T, K**]
	 *          supports three calculations: K, dK_dlog(ell), and dK_dlog(sigma_f)
	 * @note		CRTP (Curiously Recursive Template Pattern)
	 *				This class takes the corresponding covariance function class as a template.
	 *				Thus, Cov<Scalar>::f(), Cov<Scalar>::s(), Cov<Scalar>::dk_ds(), and Cov<Scalar>::ds_dell()
	 *				should be accessable from this class.
	 *				In other words, they should be public, or this class and Cov<Scalar> should be friends.
	 * @param	[in] logHyp 			The log hyperparameters, log([ell, sigma_f]).
	 * @param	[in] trainingData 	The training data.
	 * @param	[in] pdHypIndex		(Optional) Hyperparameter index.
	 * 										It returns the partial derivatives of the covariance matrix
	 * 										with respect to this hyperparameter. 
	 * 										The partial derivatives are required for learning hyperparameters.
	 * 										(Example) pdHypIndex = 0: pd[K]/pd[log(ell)], pdHypIndex = 1: pd[K]/pd[log(sigma_f)]
	 * 										(Default = -1) K
	 * @return	An NxN matrix pointer.
	 * 			N: The number of training data.
	 */
	static MatrixPtr K(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const int pdHypIndex = -1) 
	{
		// Output
		// K: (N + Nd*(D+1)) x (N + Nd*(D+1))
		// 
		// for example, when D = 3
		//                 |  F (N) | Fd (Nd) | Df1 (Nd) | Df2 (Nd) | Df3 (Nd) |
		// K = ----------------------------------------------------------------
		//        F   (N)  |    F-F |     F-Fd,     F-Df1,     F-Df2,      F-Df3
		//       --------------------------------------------------------------
		//        Fd  (Nd) |   Fd-F |    Fd-Fd,    Fd-Df1,    Fd-Df2,     Fd-Df3
		//        Df1 (Nd) |  Df1-F |   Df1-Fd,   Df1-Df1,   Df1-Df2,    Df1-Df3
		//        Df2 (Nd) |  Df2-F |   Df2-Fd,   Df2-Df1,   Df2-Df2,    Df2-Df3
		//        Df3 (Nd) |  Df3-F |   Df3-Fd,   Df3-Df1,   Df3-Df2,    Df3-Df3

		assert(pdHypIndex < logHyp.size());

		// constants		
		const int N		= derivativeTrainingData.N();		// number of function training data
		const int Nd	= derivativeTrainingData.Nd();	// number of derivative training data
		const int D		= derivativeTrainingData.D();		// number of dimensions
		const int NN	= N + Nd*(D+1);

		// memory allocation
		MatrixPtr pK(new Matrix(NN, NN));

		// fill the block matrix
		// for each row
		for(int row = 0; row < D+1; row++)
		{
			// starting row
			int startingRow;
			if(row == 0) startingRow = 0;
			else			 startingRow = N + Nd*(row-1);

			// for each column
			for(int col = 0; col < D+1; col++)
			{
				// starting column
				int startingCol;
				if(col == 0) startingCol = 0;
				else			 startingCol = N + Nd*(col-1);

				// each block matrix
				switch(row)
				{
					// first row
					case 0:
					{
						if(N <= 0) break;
						switch(col)
						{
							// first column
							case 0:
							{
								pK->block(startingRow, startingCol, N, N) = *(K(logHyp, static_cast<TrainingData<Scalar> >(derivativeTrainingData), pdIndex));
								break;
							}
							// second column
							case 1:
							{
								if(Nd <= 0) break;
								pK->block(startingRow, startingCol, N, Nd) = *(K_F_Fd(logHyp, derivativeTrainingData, pdIndex));
								break;
							}
							// other columns
							default:
							{
								if(Nd <= 0) break;
								pK->block(startingRow, startingCol, N, Nd) = *(K_F_Df(logHyp, derivativeTrainingData, col-1, pdIndex));
								break;
							}
						}
						break;
					}
					// second row
					case 1:
					{
						if(Nd <= 0) break;
						switch(col)
						{
							// first column
							case 0:
							{
								if(N <= 0) break;
								pK->block(startingRow, startingCol, Nd, N) = *(K_Fd_F(logHyp, derivativeTrainingData, pdIndex));
								break;
							}
							// second column
							case 1:
							{
								pK->block(startingRow, startingCol, Nd, Nd) = *(K_Fd_Fd(logHyp, derivativeTrainingData, pdIndex));
								break;
							}
							// other columns
							default:
							{
								pK->block(startingRow, startingCol, Nd, Nd) = *(K_Fd_Df(logHyp, derivativeTrainingData, col-1, pdIndex));
								break;
							}
						}
						break;
					}
					// other rows
					default:
					{
						if(Nd <= 0) break;
						switch(col)
						{
							// first column
							case 0:
							{
								if(N <= 0) break;
								pK->block(startingRow, startingCol, Nd, N) = *(K_Df_F(logHyp, derivativeTrainingData, row-1, pdIndex));
								break;
							}
							// second column
							case 1:
							{
								pK->block(startingRow, startingCol, Nd, Nd) = *(K_Df_Fd(logHyp, derivativeTrainingData, row-1, pdIndex));
								break;
							}
							// other columns
							default:
							{
								pK->block(startingRow, startingCol, Nd, Nd) = *(K_Df_Df(logHyp, derivativeTrainingData, row-1, col-1, pdIndex));
								break;
							}
						}
						break;
					}
					break;
				}
			}
		}

		return pK;
	}

protected:
	static MatrixPtr K_F_Fd(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const int pdIndex))
	{
		// constants		
		const int N		= derivativeTrainingData.N();		// number of function training data
		const int Nd	= derivativeTrainingData.Nd();	// number of derivative training data

		// assertion
		assert(pdHypIndex < logHyp.size());
		assert(N > 0 && Nd > 0);

		// memory allocation
		MatrixPtr pK(new Matrix(N, Nd));

		// derivative of K w.r.t a hyperparameter
		switch(pdHypIndex)
		{
			// derivative w.r.t log(ell)
			//      d            dx'        d^2 k(x, x')           d^2k    ds     ds       dk     d^2s
			// ---------- k(x, ------) = ----------------- = ell*(------*------*------- + ----*-----------)
			//  dlog(ell)       dx'_j     dlog(ell) dx'_j          ds^2   dell   dx'_j     ds   dell dx'_j 
			case 0:
			{
				// constants
				const Scalar ell = exp(logHyp(0)); // ell

				// calculation
				(*pK).noalias() = ell*(d2k_ds2(
					pK->cwiseProduct(*Cov<Scalar>::ds_dell(logHyp, trainingData)));

				return pK;	
			}

			// derivative w.r.t log(sigma_f)
	 		//	      dk             dk          dsigma_f                     dk
	      // --------------- = ---------- * --------------- = sigma_f * ---------- = 2*k
	      //  dlog(sigma_f)     dsigma_f     dlog(sigma_f)               dsigma_f
			case 1:
			{
				// memory allocation
				MatrixPtr pK = K(logHyp, trainingData);

				// dk/dlog(sigma_f) = sigma_f * k(x, x')
				(*pK).noalias() = static_cast<Scalar>(2.0) * (*pK);

				return pK;
			}
		}

		// original K

		// constants
		const Scalar sigma_f2 = exp(static_cast<Scalar>(2.0) * logHyp(1)); // sigma_f^2

		// memory allocation
		MatrixPtr pK = Cov<Scalar>::f(logHyp, trainingData);

		// k(x, x') = sigma_f^2 * f(s)
		(*pK).noalias() = sigma_f2 * (*pK);

		return pK;
	}

	static MatrixPtr K_F_Df(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int col-1, const int pdIndex));
	static MatrixPtr K_Fd_F(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const int pdIndex));
	static MatrixPtr K_Fd_Fd(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, const int pdIndex));
	static MatrixPtr K_Fd_Df(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int col-1, const int pdIndex));
	static MatrixPtr K_Df_F(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int row-1, const int pdIndex));
	static MatrixPtr K_Df_Fd(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int row-1, const int pdIndex));
	static MatrixPtr K_Df_Df(const Hyp &logHyp, DerivativeTrainingData<Scalar> &derivativeTrainingData, int row-1, int col-1, const int pdIndex));

	/**
	 * @brief	K*: Cross covariance matrix between the training data and test data.
	 * 			[K,    (K*)]: covariance matrix of the marginal Gaussian distribution 
	 * 			[(K*T), K**]
	 * @note		No pdHypIndex parameter is passed,
	 * 			because the partial derivatives of the covariance matrix
	 * 			is only required for learning hyperparameters.
	 * @param	[in] logHyp 			The log hyperparameters, log([ell, sigma_f]).
	 * @param	[in] trainingData 	The training data.
	 * @param	[in] pXs 				The test inputs.
	 * @return	An NxM matrix pointer.
	 * 			N: The number of training data.
	 * 			M: The number of test data.
	 */
	static MatrixPtr Ks(const Hyp &logHyp, TrainingData<Scalar> &trainingData, const MatrixConstPtr pXs)
	{
		// constants
		const Scalar sigma_f2 = exp(static_cast<Scalar>(2.0) * logHyp(1)); // sigma_f^2

		// memory allocation
		MatrixPtr pM = Cov<Scalar>::fs(logHyp, trainingData, pXs);

		// k(x, x') = sigma_f^2 * f(s)
		(*pM).noalias() = sigma_f2 * (*pM);

		return pM;
	}

	/**
	 * @brief	K**: Self [co]variance matrix between the test data.
	 * 			[K,    K*  ]: covariance matrix of the marginal Gaussian distribution 
	 * 			[K*T, (K**)]
	 * @note		No pdHypIndex parameter is passed,
	 * 			because the partial derivatives of the covariance matrix
	 * 			is only required for learning hyperparameters.
	 * @param	[in] logHyp 			The log hyperparameters, log([ell, sigma_f]).
	 * @param	[in] pXs 				The test inputs.
	 * @param	[in] fVarianceVector	Flag for the return value.
	 * @return	fVarianceVector == true : An Mx1 matrix pointer.
	 * 			fVarianceVector == false: An MxM matrix pointer.
	 * 			M: The number of test data.
	 */
	static MatrixPtr Kss(const Hyp &logHyp, const MatrixConstPtr pXs, const bool fVarianceVector = true)
	{
		// constants
		const Scalar sigma_f2 = exp(static_cast<Scalar>(2.0) * logHyp(1)); // sigma_f^2

		// K: self-variance vector (Mx1).
		if(fVarianceVector)
		{			
			// memory allocation
			const int M = pXs->rows();			// number of test data.
			MatrixPtr pM(new Matrix(M, 1));	// Mx1 matrix

			// k(x, x') = sigma_f^2
			pM->fill(sigma_f2);

			return pM;
		}
		else
		{
			// memory allocation
			MatrixPtr pM = Cov<Scalar>::fss(logHyp, pXs);

			// k(x, x') = sigma_f^2 * f(s)
			(*pM).noalias() = sigma_f2 * (*pM);

			return pM;
		}
	}
};

}

#endif